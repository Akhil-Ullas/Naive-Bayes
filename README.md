ğŸ“Œ What is Naive Bayes and Why is it Called â€œNaiveâ€?

Naive Bayes is a probabilistic classifier based on Bayesâ€™ Theorem.
It predicts the class with the highest posterior probability.

It is called â€œNaiveâ€ because it assumes that all features are:

conditionally independent given the target class

This assumption is unrealistic in many real-world datasets â€” especially text â€” yet Naive Bayes still performs strongly due to:

â€¢ simplicity
â€¢ fast computation
â€¢ stability on sparse, high-dimensional data

This makes it a powerful and practical baseline model.

ğŸ“Œ Variants of Naive Bayes Explored

ğŸ”¹ Gaussian Naive Bayes (GaussianNB)
Used for continuous numeric features that follow a normal distribution
Common in medical & sensor-based datasets.

It models each feature using a Gaussian (bell-curve) distribution within each class.

ğŸ”¹ Multinomial Naive Bayes (MultinomialNB)
Best suited for text classification with:

â€¢ Count Vectorizer
â€¢ Bag-of-Words
â€¢ word frequency features

It models word occurrence counts, making it ideal for:

â€¢ spam filtering
â€¢ sentiment analysis
â€¢ document classification

This was the most effective model across most text datasets I worked on.

ğŸ”¹ Bernoulli Naive Bayes (BernoulliNB)
Works with binary features such as:

â€¢ word present vs not present
â€¢ keyword indicators
â€¢ binary attribute flags

Useful when word frequency matters less than occurrence.

ğŸ“Œ Role of Count Vectorizer

Count Vectorizer converts text into numeric feature vectors by:

â€¢ mapping tokens to vocabulary indices
â€¢ representing documents as word-occurrence counts

This enabled scalable bag-of-words text representation across projects.

ğŸ“Œ Pipeline Integration

ML Pipelines were used to:

â€¢ combine preprocessing + feature extraction + model training
â€¢ avoid manual repetition
â€¢ prevent data leakage
â€¢ ensure clean and reproducible experiments

Typical workflow:

Text Preprocessing â†’ Count Vectorizer â†’ Naive Bayes Model

This strengthened my understanding of production-ready ML workflows.

âœ” Advantages of Naive Bayes

â€¢ Extremely fast to train and predict
â€¢ Performs well on high-dimensional text data
â€¢ Requires minimal training data
â€¢ Robust to irrelevant features
â€¢ Strong baseline classifier

âœ˜ Disadvantages of Naive Bayes

â€¢ Independence assumption is rarely realistic
â€¢ Can struggle when features are highly correlated
â€¢ Probability estimates may be poorly calibrated
â€¢ Sensitive to zero-frequency terms without smoothing
